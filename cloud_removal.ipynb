import kagglehub

# Download latest version
path = kagglehub.dataset_download("shubhank001/rice-remote-sensing-images-for-cloud-removal")

print("Path to dataset files:", path)
import os

dataset_path = "/root/.cache/kagglehub/datasets/shubhank001/rice-remote-sensing-images-for-cloud-removal/versions/1"

for root, dirs, files in os.walk(dataset_path):
    print(f" Folder: {root}")
    for file in files[:5]:  # Show only the first 5 files per folder
        print(f" File: {file}")
import os
import zipfile
import numpy as np
import cv2
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, Input, Activation
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
import albumentations as A
from albumentations.core.composition import OneOf
from tensorflow.keras.optimizers import Adam
from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim
from tensorflow.keras.utils import Sequence
data_path = "/root/.cache/kagglehub/datasets/shubhank001/rice-remote-sensing-images-for-cloud-removal/versions/1/RICE"
rice1_cloud_path = os.path.join(data_path, "RICE1/cloud")
rice1_label_path = os.path.join(data_path, "RICE1/label")
rice1_test_cloud_path = os.path.join(data_path, "RICE1/Test/cloud")
rice1_test_label_path = os.path.join(data_path, "RICE1/Test/label")

rice2_cloud_path = os.path.join(data_path, "RICE2/cloud")
rice2_label_path = os.path.join(data_path, "RICE2/label")
rice2_mask_path = os.path.join(data_path, "RICE2/mask")
rice2_test_cloud_path = os.path.join(data_path, "RICE2/Test/Cloud")
rice2_test_label_path = os.path.join(data_path, "RICE2/Test/Label")

img_size = (256, 256)
def load_image_paths(folder):
    return sorted([os.path.join(folder, fname) for fname in os.listdir(folder)])

# Load file paths
rice1_cloud_paths = load_image_paths(rice1_cloud_path)
rice1_label_paths = load_image_paths(rice1_label_path)
rice2_cloud_paths = load_image_paths(rice2_cloud_path)
rice2_label_paths = load_image_paths(rice2_label_path)

print(f"Loaded {len(rice1_cloud_paths)} RICE1 cloud image paths and {len(rice1_label_paths)} RICE1 label image paths")
print(f"Loaded {len(rice2_cloud_paths)} RICE2 cloud image paths and {len(rice2_label_paths)} RICE2 label image paths")

class DataGenerator(Sequence):
    def __init__(self, cloud_paths, label_paths, batch_size=16, augment=True):
        self.cloud_paths = cloud_paths
        self.label_paths = label_paths
        self.batch_size = batch_size
        self.augment = augment
        self.augmentations = A.Compose([
            OneOf([
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.5),
                A.RandomRotate90(p=0.5)
            ], p=1),
            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),
            A.RandomBrightnessContrast(p=0.2)
        ])

    def __len__(self):
      return max(1, int(np.floor(len(self.cloud_paths) / self.batch_size)))  # Ensures minimum 1 batch



    def __getitem__(self, index):
        batch_cloud_paths = self.cloud_paths[index * self.batch_size:(index + 1) * self.batch_size]
        batch_label_paths = self.label_paths[index * self.batch_size:(index + 1) * self.batch_size]
        cloud_imgs, label_imgs = [], []
        for c_path, l_path in zip(batch_cloud_paths, batch_label_paths):
            cloud_img = cv2.imread(c_path)
            label_img = cv2.imread(l_path)
            if cloud_img is not None and label_img is not None:
                cloud_img = cv2.resize(cloud_img, img_size) / 255.0
                label_img = cv2.resize(label_img, img_size) / 255.0
                if self.augment:
                    augmented = self.augmentations(image=cloud_img)
                    cloud_img = augmented["image"]
                cloud_imgs.append(cloud_img)
                label_imgs.append(label_img)
        return np.array(cloud_imgs), np.array(label_imgs)

def build_generator():
    inputs = Input(shape=(256, 256, 3))
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = Conv2DTranspose(64, (3, 3), padding='same', activation='relu')(x)
    x = Conv2D(3, (3, 3), padding='same', activation='sigmoid')(x)
    x = BatchNormalization()(x)

    return Model(inputs, x)
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, Flatten, Dense, Dropout

def build_discriminator():
    inputs = Input(shape=(256, 256, 3))

    x = Conv2D(64, (3, 3), strides=2, padding='same')(inputs)
    x = LeakyReLU(alpha=0.2)(x)
    x = Dropout(0.4)(x)  # Adding dropout

    x = Conv2D(128, (3, 3), strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Dropout(0.4)(x)  # Adding dropout

    x = Flatten()(x)
    x = Dense(1, activation="sigmoid")(x)

    return Model(inputs, x)
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, Input, Activation, Flatten, Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
# Compile models
generator = build_generator()
discriminator = build_discriminator()

# Compile both models
generator.compile(optimizer=Adam(0.0002, 0.5), loss='mae')  # Fix: Compile generator
discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])
generator.compile(optimizer=Adam(0.0002, 0.5), loss='mae')  # Compile generator

gan_model.compile(loss="binary_crossentropy", optimizer="adam")

import numpy as np

def train_gan(generator, discriminator, gan_model, train_generator, epochs=100, batch_size=32):
    steps_per_epoch = len(train_generator)

    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        d_loss_real_total, d_loss_fake_total, g_loss_total = 0, 0, 0  # Initialize loss accumulators

        for step in range(steps_per_epoch):
            batch_clouds, batch_labels = train_generator[step]

            # Generate fake images
            fake_imgs = generator.predict(batch_clouds)

            # Add small noise to stabilize training
            batch_labels += np.random.normal(0, 0.05, batch_labels.shape)
            fake_imgs += np.random.normal(0, 0.05, fake_imgs.shape)

            # Real & Fake Labels (Slightly Noisy)
            real_labels = np.ones((batch_size, 1)) * 0.9  # Label smoothing for real data
            fake_labels = np.zeros((batch_size, 1))  # Fake labels are zero

            # Train Discriminator on Real and Fake Data
            d_loss_real = discriminator.train_on_batch(batch_labels, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)

            # Ensure that `train_on_batch` returns a single scalar loss value
            if isinstance(d_loss_real, (list, tuple)):
                d_loss_real = d_loss_real[0]
            if isinstance(d_loss_fake, (list, tuple)):
                d_loss_fake = d_loss_fake[0]

            # Accumulate discriminator losses
            d_loss_real_total += d_loss_real
            d_loss_fake_total += d_loss_fake

            # Freeze Discriminator while training Generator
            discriminator.trainable = False

            # Train Generator (We use `1s` as labels because we want it to fool the discriminator)
            misleading_labels = np.ones((batch_size, 1))

            # Update generator multiple times per step
            g_loss = gan_model.train_on_batch(batch_clouds, misleading_labels)

            # Ensure `g_loss` is a scalar value
            if isinstance(g_loss, (list, tuple)):
                g_loss = g_loss[0]

            g_loss_total += g_loss

            discriminator.trainable = True  # Unfreeze Discriminator

        # Compute average losses per epoch
        avg_d_loss = (d_loss_real_total + d_loss_fake_total) / (2 * steps_per_epoch)
        avg_g_loss = g_loss_total / steps_per_epoch  # Generator loss is already averaged

        print(f"Epoch {epoch+1}: D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}")

# Train the GAN
train_gan(generator, discriminator, gan_model, train_generator, epochs=10, batch_size=32)
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os

# Create a folder to save generated images
output_folder = "/content/generated_images"
os.makedirs(output_folder, exist_ok=True)

def generate_display_save_images(generator, test_image_paths, save_folder):
    """
    Generates cloud-free images using the trained generator, displays them, and saves them.

    Args:
        generator: Trained GAN generator model.
        test_image_paths: List of test image file paths.
        save_folder: Folder where generated images will be saved.
    """
    num_images = len(test_image_paths)
    plt.figure(figsize=(num_images * 5, 5))

    for i, img_path in enumerate(test_image_paths):
        if not os.path.exists(img_path):
            print(f"❌ Image not found: {img_path}")
            continue

        # Load and preprocess image
        img = cv2.imread(img_path)
        img = cv2.resize(img, (256, 256)) / 255.0  # Normalize
        img = np.expand_dims(img, axis=0)  # Add batch dimension

        # Generate cloud-free image
        generated_img = generator.predict(img)[0]

        # Convert back to uint8 format for saving
        generated_img_uint8 = (generated_img * 255).astype(np.uint8)

        # Save generated image
        filename = os.path.basename(img_path)
        save_path = os.path.join(save_folder, f"generated_{filename}")
        cv2.imwrite(save_path, cv2.cvtColor(generated_img_uint8, cv2.COLOR_RGB2BGR))  # Save in correct format
        print(f"✅ Saved: {save_path}")

        # Display the original and generated images side by side
        plt.subplot(2, num_images, i + 1)  # Top row: Original images
        plt.imshow(img[0])
        plt.title(f"Cloudy: {filename}")
        plt.axis("off")

        plt.subplot(2, num_images, i + 1 + num_images)  # Bottom row: Generated images
        plt.imshow(generated_img)
        plt.title(f"Generated: {filename}")
        plt.axis("off")

    plt.tight_layout()
    plt.show()

# Example: Use multiple test images from the RICE dataset
test_images = [
    os.path.join(rice1_test_cloud_path, "1.png"),
    os.path.join(rice1_test_cloud_path, "102.png"),
    os.path.join(rice1_test_cloud_path, "495.png"),
    os.path.join(rice1_test_cloud_path, "487.png"),
    os.path.join(rice1_test_cloud_path, "256.png")
]

generate_display_save_images(generator, test_images, output_folder)

